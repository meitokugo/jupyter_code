{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loaded the word list!\n"
     ]
    }
   ],
   "source": [
    "wordsList = np.load('./No_17/training_data/wordsList.npy')\n",
    "print('loaded the word list!')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded the word vectors!\n"
     ]
    }
   ],
   "source": [
    "wordsList = wordsList.tolist() #Originally loaded as numpy array\n",
    "             #将单词编码为UTF-8格式\n",
    "wordsList = [word.decode('UTF-8') for word in wordsList]\n",
    "wordVectors = np.load('./No_17/training_data/wordVectors.npy')\n",
    "print('Loaded the word vectors!')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "400000\n",
      "(400000, 50)\n"
     ]
    }
   ],
   "source": [
    "print(len(wordsList))\n",
    "print(wordVectors.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([-1.9327  ,  1.0421  , -0.78515 ,  0.91033 ,  0.22711 , -0.62158 ,\n",
       "       -1.6493  ,  0.07686 , -0.5868  ,  0.058831,  0.35628 ,  0.68916 ,\n",
       "       -0.50598 ,  0.70473 ,  1.2664  , -0.40031 , -0.020687,  0.80863 ,\n",
       "       -0.90566 , -0.074054, -0.87675 , -0.6291  , -0.12685 ,  0.11524 ,\n",
       "       -0.55685 , -1.6826  , -0.26291 ,  0.22632 ,  0.713   , -1.0828  ,\n",
       "        2.1231  ,  0.49869 ,  0.066711, -0.48226 , -0.17897 ,  0.47699 ,\n",
       "        0.16384 ,  0.16537 , -0.11506 , -0.15962 , -0.94926 , -0.42833 ,\n",
       "       -0.59457 ,  1.3566  , -0.27506 ,  0.19918 , -0.36008 ,  0.55667 ,\n",
       "       -0.70315 ,  0.17157 ], dtype=float32)"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "baseballIndex = wordsList.index('baseball')\n",
    "wordVectors[baseballIndex]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(10,)\n",
      "[    41    804 201534   1005     15   7446      5  13767      0      0]\n"
     ]
    }
   ],
   "source": [
    "#词向量矩阵\n",
    "maxSeqLength = 10    #单词最大长度\n",
    "numSeqLength = 300  #单词向量维度 \n",
    "firstSentence = np.zeros((maxSeqLength),dtype='int32')\n",
    "firstSentence[0] = wordsList.index('i')\n",
    "firstSentence[1] = wordsList.index('thought')\n",
    "firstSentence[2] = wordsList.index('the')\n",
    "firstSentence[3] = wordsList.index('movie')\n",
    "firstSentence[4] = wordsList.index('was')\n",
    "firstSentence[5] = wordsList.index('incredible')\n",
    "firstSentence[6] = wordsList.index('and')\n",
    "firstSentence[7] = wordsList.index('inspiring')\n",
    "firstSentence[8] = wordsList.index('0')\n",
    "firstSentence[9] = wordsList.index('0')\n",
    "print(firstSentence.shape)\n",
    "print(firstSentence)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(10, 50)\n"
     ]
    }
   ],
   "source": [
    "import tensorflow.compat.v1 as tf\n",
    "tf.disable_v2_behavior()\n",
    "with tf.Session() as sess:\n",
    "    print(tf.nn.embedding_lookup(wordVectors,firstSentence).eval().shape)\n",
    "#结果说明输出数据是一个10*50的词矩阵，其中有10个词，每个词的向量维度是50。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "#构造索引之前，可视化所拥有的数据类型，有助于我们决定如何设置最大序列长度的最佳值，"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Positive files finished\n",
      "Negative files finished\n",
      "The total number of files is 25000\n",
      "The average number of words in the files is 233.7872\n"
     ]
    }
   ],
   "source": [
    "#导入一共25000条电影数据，12500条正向数据和12500条负向数据，并解析这些文件\n",
    "from os import listdir\n",
    "from os.path import isfile, join\n",
    "positiveFiles = ['./No_17/training_data/positiveReviews/' + f for f in listdir('./No_17/training_data/positiveReviews/') if isfile(join('./No_17/training_data/positiveReviews/',f))]\n",
    "negativeFiles = ['./No_17/training_data/negativeReviews/' + f for f in listdir('./No_17/training_data/negativeReviews/') if isfile(join('./No_17/training_data/negativeReviews/',f))]\n",
    "numWords=[]\n",
    "for pf in positiveFiles:\n",
    "    with open(pf, \"r\",encoding='utf-8') as f:\n",
    "        line = f.readline()\n",
    "        counter = len(line.split())\n",
    "        numWords.append(counter)\n",
    "print('Positive files finished')\n",
    "\n",
    "for nf in negativeFiles:\n",
    "    with open(nf,\"r\",encoding='utf-8') as f:\n",
    "        line= f.readline()\n",
    "        counter= len(line.split())\n",
    "        numWords.append(counter)\n",
    "print('Negative files finished')\n",
    "\n",
    "numFiles = len(numWords)\n",
    "print('The total number of files is',numFiles)\n",
    "print('The average number of words in the files is',sum(numWords)/len(numWords))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "#使用matplot对数据进行可视化\n",
    "#import matplotlib.pyplot as plt\n",
    "#import matplotlib.font_manager as fm\n",
    "#myfont = fm.FontProperties(fname='/home/wumg/anaconda3/lib/python3.6/site-packages/matplotlib/mpl-data/fonts/ttf/simhei.ttf')\n",
    "#%matplotlib inline\n",
    "#plt.hist(numWords,50)\n",
    "#plt.xlabel('序列长度',fontproperties=myfont)\n",
    "#plt.ylabel('频度',fontproperties=myfont)\n",
    "#plt.xaxos([0,1200,0,8000])\n",
    "#plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "#上述可视化目的是为了找出平均单词，找出合适的句子最大长度，\n",
    "#下面将单个文件中的文本转换变成索引矩阵。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "This isn't the comedic Robin Williams, nor is it the quirky/insane Robin Williams of recent thriller fame. This is a hybrid of the classic drama without over-dramatization, mixed with Robin's new love of the thriller. But this isn't a thriller, per se. This is more a mystery/suspense vehicle through which Williams attempts to locate a sick boy and his keeper.<br /><br />Also starring Sandra Oh and Rory Culkin, this Suspense Drama plays pretty much like a news report, until William's character gets close to achieving his goal.<br /><br />I must say that I was highly entertained, though this movie fails to teach, guide, inspect, or amuse. It felt more like I was watching a guy (Williams), as he was actually performing the actions, from a third person perspective. In other words, it felt real, and I was able to subscribe to the premise of the story.<br /><br />All in all, it's worth a watch, though it's definitely not Friday/Saturday night fare.<br /><br />It rates a 7.7/10 from...<br /><br />the Fiend :.\n"
     ]
    }
   ],
   "source": [
    "maxSeqLength = 250\n",
    "fname = positiveFiles[5] #can use any valid index(not just 3)\n",
    "with open(fname) as f:\n",
    "    for lines in f:\n",
    "        print(lines)\n",
    "        exit\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([399999,      0,      0,      0,      0,      0,      0,      0,\n",
       "            0,      0,      0,      0,      0,      0,      0,      0,\n",
       "            0,      0,      0,      0,      0,      0,      0,      0,\n",
       "            0,      0,      0,      0,      0,      0,      0,      0,\n",
       "            0,      0,      0,      0,      0,      0,      0,      0,\n",
       "            0,      0,      0,      0,      0,      0,      0,      0,\n",
       "            0,      0,      0,      0,      0,      0,      0,      0,\n",
       "            0,      0,      0,      0,      0,      0,      0,      0,\n",
       "            0,      0,      0,      0,      0,      0,      0,      0,\n",
       "            0,      0,      0,      0,      0,      0,      0,      0,\n",
       "            0,      0,      0,      0,      0,      0,      0,      0,\n",
       "            0,      0,      0,      0,      0,      0,      0,      0,\n",
       "            0,      0,      0,      0,      0,      0,      0,      0,\n",
       "            0,      0,      0,      0,      0,      0,      0,      0,\n",
       "            0,      0,      0,      0,      0,      0,      0,      0,\n",
       "            0,      0,      0,      0,      0,      0,      0,      0,\n",
       "            0,      0,      0,      0,      0,      0,      0,      0,\n",
       "            0,      0,      0,      0,      0,      0,      0,      0,\n",
       "            0,      0,      0,      0,      0,      0,      0,      0,\n",
       "            0,      0,      0,      0,      0,      0,      0,      0,\n",
       "            0,      0,      0,      0,      0,      0,      0,      0,\n",
       "            0,      0,      0,      0,      0,      0,      0,      0,\n",
       "            0,      0,      0,      0,      0,      0,      0,      0,\n",
       "            0,      0,      0,      0,      0,      0,      0,      0,\n",
       "            0,      0,      0,      0,      0,      0,      0,      0,\n",
       "            0,      0,      0,      0,      0,      0,      0,      0,\n",
       "            0,      0,      0,      0,      0,      0,      0,      0,\n",
       "            0,      0,      0,      0,      0,      0,      0,      0,\n",
       "            0,      0,      0,      0,      0,      0,      0,      0,\n",
       "            0,      0,      0,      0,      0,      0,      0,      0,\n",
       "            0,      0,      0,      0,      0,      0,      0,      0,\n",
       "            0,      0])"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#把他转换成索引矩阵\n",
    "#删除标点括号问号，只留下字母数字\n",
    "import re \n",
    "strip_special_chars = re.compile(\"[^A-Za-z0-9]+\")\n",
    "\n",
    "def cleanSentences(string):\n",
    "    string = string.lower().replace(\"<br />\",\" \")\n",
    "    return re.sub(strip_special_chars,\"\",string.lower())\n",
    "\n",
    "firstFile = np.zeros((maxSeqLength),dtype='int32')\n",
    "\n",
    "with open(fname) as f:\n",
    "    indexCounter = 0\n",
    "    line=f.readline()\n",
    "    cleanedLine = cleanSentences(line)\n",
    "    split = cleanedLine.split()\n",
    "    for word in split:\n",
    "        try:\n",
    "            firstFile[indexCounter] = wordsList.index(word)\n",
    "        except ValueError:\n",
    "            firstFile[indexCounter] = 399999 #Vector for unknown words\n",
    "        indexCounter = indexCounter + 1\n",
    "firstFile"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "#利用相同方法，处理全部25000条论文，导入电影训练集，并得到25000x250的矩阵，采用迁移学习的方法，直接使用训练好的索引矩阵文件，\n",
    "ids = np.load('./No_17/training_Data/idsMatrix.npy')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "#定义辅助函数，在训练神经网络步骤需要\n",
    "\n",
    "from random import randint\n",
    "\n",
    "def getTrainBatch():\n",
    "    labels= []\n",
    "    arr = np.zeros([batchSize,MaxSeqLength])\n",
    "    for i in range(batchSize):\n",
    "        if (i%2 == 0 ):\n",
    "            num = randint(1,11499)\n",
    "            labels.append([1,0])\n",
    "        else:\n",
    "            num = randint(13299,24999)\n",
    "            labels.append([0,1])\n",
    "        arr[i] = ids[num-1:num]\n",
    "    return arr,labels\n",
    "\n",
    "def getTestBatch():\n",
    "    labels = []\n",
    "    arr = np.zeros([batchSize,maxSeqLength])\n",
    "    for i in range(batchSize):\n",
    "        num = randint(11499,13499)\n",
    "        if (num <=12499 ):\n",
    "            labels.append([1,0])\n",
    "        else:\n",
    "            labels.append([0,1])\n",
    "        arr[i] = ids[num-1:num]\n",
    "    return arr,labels\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "#构建RNN模型，在构建图模型前，先定义一个超参数，批处理大小，LSTM单元个数，分类类别，训练次数\n",
    "batchSize=24\n",
    "lstmUnits=64\n",
    "numClasses = 2\n",
    "iterations = 20000"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "#指定两个占位符，一个用于数据输入，另一个用来标签数据，\n",
    "import tensorflow.compat.v1 as tf\n",
    "from tensorflow.python.framework import ops\n",
    "tf.compat.v1.disable_eager_execution()\n",
    "ops.reset_default_graph()\n",
    "labels = tf.placeholder(tf.float32,[batchSize,numClasses])\n",
    "input_data = tf.placeholder(tf.int32,[batchSize,maxSeqLength])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = tf.nn.embedding_lookup(wordVectors,input_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "module 'tensorflow_core.compat.v1' has no attribute 'contrib'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-33-2e0da997ee9b>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      7\u001b[0m \u001b[0;31m#import tensorflow.compat.v1 as tf\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      8\u001b[0m \u001b[0;31m#tf.disable_v2_behavior()\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 9\u001b[0;31m \u001b[0mlstmCell\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcontrib\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrnn\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mBasicLSTMCell\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlstmUnits\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     10\u001b[0m \u001b[0mlstmCell\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcontrib\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrnn\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mDropoutWrapper\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcell\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mlstmCell\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0moutput_keep_prob\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m0.25\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     11\u001b[0m \u001b[0mvalue\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0m_\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnn\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdynamic_rnn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlstmCell\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mdtype\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mtf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfloat32\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mAttributeError\u001b[0m: module 'tensorflow_core.compat.v1' has no attribute 'contrib'"
     ]
    }
   ],
   "source": [
    "#构建RNN模型\n",
    "#因为教程使用tf1.6所以此段构建RNN模型函数失败。\n",
    "import tensorflow.compat.v1 as tf\n",
    "tf.disable_v2_behavior()\n",
    "\n",
    "# -outputs, states = tf.contrib.rnn.static_rnn(lstm_cells, _X, dtype=tf.float32)\n",
    "outputs, states = tf.compat.v1.nn.static_rnn(lstm_cells, _X, dtype=tf.float32)\n",
    "#import tensorflow.compat.v1 as tf\n",
    "#tf.disable_v2_behavior()\n",
    "lstmCell = tf.contrib.rnn.BasicLSTMCell(lstmUnits)\n",
    "lstmCell = tf.contrib.rnn.DropoutWrapper(cell=lstmCell,output_keep_prob=0.25)\n",
    "value,_ = tf.nn.dynamic_rnn(lstmCell,data,dtype=tf.float32)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "module 'tensorflow_core.compat.v1' has no attribute 'truncated_narmal'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-34-7a7424e4631e>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;31m#dynamic RNN 函数第一个输出被认为是最后隐藏状态向量，被重新定义维度，然后乘以最高的权重矩阵和偏置项后获得最终输出。\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0mweight\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mVariable\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtruncated_narmal\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mlstmUnits\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mnumClasses\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      3\u001b[0m \u001b[0mbiias\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mVariable\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mconstant\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m0.1\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mnumClasses\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0mvalue\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtranspose\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mvalue\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m2\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0mlast\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgather\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mvalue\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mvalu\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_shape\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mAttributeError\u001b[0m: module 'tensorflow_core.compat.v1' has no attribute 'truncated_narmal'"
     ]
    }
   ],
   "source": [
    "#dynamic RNN 函数第一个输出被认为是最后隐藏状态向量，被重新定义维度，然后乘以最高的权重矩阵和偏置项后获得最终输出。\n",
    "weight = tf.Variable(tf.truncated_narmal([lstmUnits,numClasses]))\n",
    "biias = tf.Variable(tf.constant(0.1,shape=[numClasses]))\n",
    "value = tf.transpose(value,[1,0,2])\n",
    "last = tf.gather(value,int(valu.get_shape()[0])-1)\n",
    "prediction = (tf.matmul(last,weight)+bias)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "correctPred = tf.equal(tf.argmax(prediction,1),tf.argmax(labels,1))\n",
    "accuracy = tf.reduce_mean(tf.cast(correctPred,tf.float32))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "loss = tf.reduce_mean(tf.nn.soft_cross_entropy_with_logits_v2(logits=prediction,labels=labels))\n",
    "optimizer = tf.train.AdamOptimizer().minimize(loss)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#使用tensorboard来可视化损失值和准确率\n",
    "import datetime\n",
    "tf.summary.scalar('Loss',loss)\n",
    "tf.summary.scalar('Accuracy',accuracy)\n",
    "merged = tf.summary.merge_all()\n",
    "logdir = \"tensorboard/\"+datetime.datetime.now().strftime(\"%Y%m%d-%H%M%S\") +\"/\"\n",
    "writer = tf.summary.FileWriter(logdir,sess.graph)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#模型训练\n",
    "#定义一个Tensorflow 会话，然后加入一批评论和对应的标签，\n",
    "#调用run函数，run函数有两个参数，第一个叫fetches，定义了我们感兴趣的值，希望通过优化器来优化最小代价函数，\n",
    "#第二个函数叫feed_dict参数，这个数据结构就是占位符，\n",
    "sess = tf.InteractiveSession()\n",
    "saver = tf.train.Saver()\n",
    "sess.run(tf.global_variables_initializer())\n",
    "with tf.device('/gpu:0'):\n",
    "    for i in range(iterations):\n",
    "        #获取下一批数据\n",
    "        nextBatch,nextBatchLabels = getTrainBatch();\n",
    "        sess.run(optimizer,{input_data:nextBatch,labels:nextBatchLabels})\n",
    "        #汇总信息写入Tensorboard\n",
    "        if (i%50 == 0):\n",
    "            summary = sess.run(merged,{input_data:nextBatch,labels:nextBatchLabels})\n",
    "            writer.add_summary(summary,i)\n",
    "        #每训练100次保存一次\n",
    "        if (i/1000 == 0 and i != 0):\n",
    "            save_path = saver.save(sess,\"models/pretrained_lstm.ckpt\",global_step=i)\n",
    "            print(\"saved to %s\" % save_path)\n",
    "    writer.close()\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#通过代码查看批处理的准确性，\n",
    " iterations = 10\n",
    "    for i in range(iterations):\n",
    "        nextBatch,nextBatchLabels = getTestBatch();\n",
    "        print(\"Accuracy for this batch:\",(sess.run(accuracy,{input_data:nextBatch,labels:nextBatchLabels}))*100)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
